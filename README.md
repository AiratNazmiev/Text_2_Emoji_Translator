## Модель для перевода коротких текстовых сообщений в эмодзи

Модель сделана на основе BART-base модели, дообученной на парах текст-набор эмодзи, взятых из открытых источников и сгенерированных с помощью LLM (Chat-GPT и DeepSeek). 
BART-base (Bidirectional and Auto-Regressive Transformers) — это трансформерная модель от Meta, предназначенная  для обработки естественного языка. Она объединяет идеи BERT (двунаправленного кодировщика) и GPT (авторегрессионного декодера) в архитектуре encoder–decoder, аналогичной трансформеру.

Упор в обучении был полностью сделан на английский язык. Запросы, кроме английского, доступны на руссклм и китайском. Для этого введенный пользователем текст предварительно переводится с ипользованием модели Helsinki-NLP/opus-mt-ru-en и Helsinki-NLP/opus-mt-zh-en.
Переводчики также имеют BART-подобную архитекуру, специализированную на машинном переводе (MarianMT, обученная на OPUS). 

Дообучение производилось с ипользованием LoRA с r=4 для все блоков BART-base модели, отметим, что эмодзи входят в словарь токенов BART-подобных языковых моделей (см. **model_uptrain.ipynb**). Но, само собой, из кооробки моель не может выдавать хорошие описания из-за редкости пар текст-эмодзи в открытых источниках. В результате обучения полученная модель генерирует занимательные эмодзи-описания введенного сообщения, при этом одинаково хорошо на всех трех языках. При генерации используется beam search.

Визуализацию и тестирование модели на трех языках можно найти в **test_text2emoji.ipynb**.

Модель была развернута на сервере streamlit, с понятым и аккуратным интерфейсом. Инференс модели быстрый, кеширование данных и модели производится правильно. Весь код приложения доступен в **app.py**.

Ссылка на streanlit: https://text2emojitranslator.streamlit.app/
