{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "59cdbd3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import re\n",
    "\n",
    "import typing as tp\n",
    "\n",
    "import os\n",
    "import torchview\n",
    "import torchinfo\n",
    "\n",
    "from transformers import (\n",
    "    BartTokenizer, BartForConditionalGeneration,\n",
    "    pipeline, \n",
    "    AutoModelForSeq2SeqLM, Trainer, TrainingArguments,\n",
    ")\n",
    "import datasets\n",
    "\n",
    "import peft\n",
    "\n",
    "import evaluate\n",
    "\n",
    "np.random.seed(0)\n",
    "torch.manual_seed(0);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2d3a484c",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "304d64a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = BartTokenizer.from_pretrained(\"AiratNazmiev/text2emoji_tokenizer\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a79f5aa5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(example):\n",
    "    model_inputs = tokenizer(\n",
    "        example[\"text\"],\n",
    "        truncation=True,\n",
    "    )\n",
    "    \n",
    "    labels = tokenizer(\n",
    "        text_target=example[\"emoji\"],\n",
    "        truncation=True,\n",
    "    )\n",
    "    \n",
    "    model_inputs[\"labels\"] = labels[\"input_ids\"]\n",
    "    \n",
    "    return model_inputs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7127022a",
   "metadata": {},
   "source": [
    "Let's combine random subset of big dataset with high-quality examples generated by `gpt` and `deepseek` models in multiple rounds, their outputs make up several csv files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b5676d27",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_root = 'data'\n",
    "\n",
    "text2emoji_dataset = pd.read_csv(os.path.join(data_root, 'text2emoji.csv'))\n",
    "text2emoji_dataset.dropna(inplace=True)\n",
    "text2emoji_dataset.drop('topic', axis=1, inplace=True)\n",
    "selected_ids = np.random.choice(len(text2emoji_dataset), size=20000, replace=False)\n",
    "text2emoji_selected_dataset = text2emoji_dataset.iloc[selected_ids]\n",
    "text2emoji_selected_dataset.to_csv(os.path.join(data_root, 'text2emoji_clean.csv'), index=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "707fa426",
   "metadata": {},
   "outputs": [],
   "source": [
    "generated_root = os.path.join('data', 'generated')\n",
    "\n",
    "text2emoji_dataset_list = [text2emoji_selected_dataset]\n",
    "\n",
    "for generated_path in os.listdir(generated_root):\n",
    "    text2emoji_gen_dataset = pd.read_csv(os.path.join(generated_root, generated_path))\n",
    "    text2emoji_gen_dataset.dropna(inplace=True)\n",
    "    text2emoji_dataset_list.append(text2emoji_gen_dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55dda222",
   "metadata": {},
   "source": [
    "New dataset that will be used during training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "211ee6de",
   "metadata": {},
   "outputs": [],
   "source": [
    "text2emoji_dataset_final = pd.concat(text2emoji_dataset_list)\n",
    "text2emoji_dataset_final.reset_index(inplace=True, drop=True)\n",
    "text2emoji_dataset_final.to_csv(os.path.join(data_root, 'text2emoji_generated.csv'), index=None)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd5cab1b",
   "metadata": {},
   "source": [
    "We would better download dataset from HF hub, uploaded by me:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "46efbb7f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(Dataset({\n",
       "     features: ['text', 'emoji'],\n",
       "     num_rows: 28329\n",
       " }),\n",
       " Dataset({\n",
       "     features: ['text', 'emoji'],\n",
       "     num_rows: 12141\n",
       " }))"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "base_dataset = datasets.load_dataset('AiratNazmiev/text2emoji_generated')['train']\n",
    "val_ratio = 0.3\n",
    "split_dataset = base_dataset.train_test_split(test_size=val_ratio, seed=0)\n",
    "train_dataset = split_dataset[\"train\"]\n",
    "val_dataset = split_dataset[\"test\"]\n",
    "train_dataset, val_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "6f065652",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['text', 'emoji'],\n",
       "    num_rows: 40470\n",
       "})"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "base_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "229f8620",
   "metadata": {},
   "outputs": [],
   "source": [
    "if not os.path.exists('text2emoji_tokenized_train') or not os.path.exists('text2emoji_tokenized_val'):\n",
    "    tokenized_val_dataset = val_dataset.map(preprocess, batched=False)#, remove_columns=['text', 'emoji'])\n",
    "    tokenized_train_dataset = train_dataset.map(preprocess, batched=False)\n",
    "    tokenized_val_dataset.save_to_disk('text2emoji_tokenized_val')\n",
    "    tokenized_train_dataset.save_to_disk('text2emoji_tokenized_train')\n",
    "else:\n",
    "    tokenized_val_dataset = datasets.load_from_disk('text2emoji_tokenized_val')\n",
    "    tokenized_train_dataset = datasets.load_from_disk('text2emoji_tokenized_train')\n",
    "    \n",
    "# tokenized_train_dataset = tokenized_train_dataset.remove_columns(['text', 'emoji'])\n",
    "# tokenized_val_dataset = tokenized_val_dataset.remove_columns(['text', 'emoji'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "f9e1231b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['text', 'emoji', 'input_ids', 'attention_mask', 'labels'],\n",
       "    num_rows: 12141\n",
       "})"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenized_val_dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "190fd3b8",
   "metadata": {},
   "source": [
    "We will use LoRA for model uptraining"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "565cc27d",
   "metadata": {},
   "outputs": [],
   "source": [
    "lora_config = peft.LoraConfig(\n",
    "    task_type=\"SEQ_2_SEQ_LM\",\n",
    "    r=4,\n",
    "    lora_alpha=8,\n",
    "    target_modules=[\"k_proj\", \"v_proj\", \"q_proj\", \"out_proj\", \"fc1\", \"fc2\"],\n",
    "    lora_dropout=0.01,\n",
    "    init_lora_weights='olora',\n",
    "    lora_bias=False\n",
    ")\n",
    "\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(\"AiratNazmiev/text2emoji-bart-base\")\n",
    "lora_model = peft.get_peft_model(model, lora_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "cf391209",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of parameters: 141.94M\n",
      "Lora ft parameters: 0.81M\n"
     ]
    }
   ],
   "source": [
    "pn = 0\n",
    "tpn = 0\n",
    "\n",
    "for p in lora_model.parameters():\n",
    "    if p.requires_grad:\n",
    "        tpn += p.numel()\n",
    "    pn += p.numel()\n",
    "    \n",
    "print(f\"Total number of parameters: {pn/10**6:.2f}M\\nLora ft parameters: {tpn/10**6:.2f}M\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "bb018280",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./model_ft\",\n",
    "    overwrite_output_dir=True,\n",
    "    num_train_epochs=5,\n",
    "    learning_rate=5e-5,\n",
    "    lr_scheduler_type=\"cosine\",\n",
    "    warmup_steps=200,\n",
    "    per_device_train_batch_size=1,\n",
    "    per_device_eval_batch_size=1,\n",
    "    gradient_accumulation_steps=64,\n",
    "    log_level=\"error\",\n",
    "    logging_strategy=\"steps\",\n",
    "    logging_steps=1,\n",
    "    save_strategy=\"epoch\",\n",
    "    save_steps=1,\n",
    "    save_total_limit=2,\n",
    "    save_safetensors=True,\n",
    "    save_only_model=False,\n",
    "    use_cpu=False,\n",
    "    seed=42,\n",
    "    bf16=True,\n",
    "    eval_strategy=\"epoch\",\n",
    "    eval_steps=1,\n",
    "    disable_tqdm=False,\n",
    "    load_best_model_at_end=False,\n",
    "    label_smoothing_factor=0.,\n",
    "    optim=\"adamw_torch\",\n",
    "    remove_unused_columns=False,\n",
    "    #max_grad_norm=1.\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f3fde6d",
   "metadata": {},
   "source": [
    "We'll track accuracy as validation metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "ab0b5d48",
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy_metrics = evaluate.load(\"accuracy\")\n",
    "\n",
    "def compute_metrics(eval_pred):\n",
    "    logits, labels = eval_pred\n",
    "    predictions = np.argmax(logits, axis=-1)\n",
    "    return accuracy_metrics.compute(predictions=predictions, references=labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "85576ba3",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = Trainer(\n",
    "    model=lora_model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_train_dataset,\n",
    "    eval_dataset=tokenized_val_dataset,\n",
    "    compute_metrics=compute_metrics,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfa69845",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03cbddf3",
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_model = lora_model.merge_and_unload()\n",
    "merged_model.save_pretrained('merged_model')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python312",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
