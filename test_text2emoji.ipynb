{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8bba0c76",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import re\n",
    "\n",
    "import typing as tp\n",
    "\n",
    "import torchview\n",
    "\n",
    "import transformers\n",
    "from transformers import BartTokenizer, BartForConditionalGeneration, pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0081eb25",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<function transformers.pipelines.pipeline(task: str = None, model: Union[str, ForwardRef('PreTrainedModel'), ForwardRef('TFPreTrainedModel'), NoneType] = None, config: Union[str, transformers.configuration_utils.PretrainedConfig, NoneType] = None, tokenizer: Union[str, transformers.tokenization_utils.PreTrainedTokenizer, ForwardRef('PreTrainedTokenizerFast'), NoneType] = None, feature_extractor: Union[str, ForwardRef('SequenceFeatureExtractor'), NoneType] = None, image_processor: Union[str, transformers.image_processing_utils.BaseImageProcessor, NoneType] = None, processor: Union[str, transformers.processing_utils.ProcessorMixin, NoneType] = None, framework: Optional[str] = None, revision: Optional[str] = None, use_fast: bool = True, token: Union[str, bool, NoneType] = None, device: Union[int, str, ForwardRef('torch.device'), NoneType] = None, device_map=None, torch_dtype=None, trust_remote_code: Optional[bool] = None, model_kwargs: Dict[str, Any] = None, pipeline_class: Optional[Any] = None, **kwargs) -> transformers.pipelines.base.Pipeline>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "transformers.pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c9ed7279",
   "metadata": {},
   "outputs": [],
   "source": [
    "text2emoji_dataset = pd.read_csv('data/text2emoji.csv')\n",
    "text2emoji_dataset.dropna(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "06440d2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "text_list = text2emoji_dataset['text'].to_list()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "18df7c94",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Msg2EmojiTranslator:\n",
    "    def __init__(\n",
    "        self,\n",
    "        tokenizer,\n",
    "        generator,\n",
    "        device: torch.device\n",
    "    ) -> None:\n",
    "        self.device = device\n",
    "        self.tokenizer = tokenizer\n",
    "        self.generator = generator.to(self.device)\n",
    "        \n",
    "    def translate(self, sentence: str | list[str], sep: str = '.', **kwargs) -> torch.Tensor:\n",
    "        decoded_emojis_list = []\n",
    "        \n",
    "        if isinstance(sentence, str):\n",
    "            sentence = [sentence]\n",
    "\n",
    "        for s in sentence:\n",
    "            text_tokens = self.tokenizer(s, return_tensors=\"pt\")\n",
    "            generated_emoji_tokens = self.generator.generate(text_tokens[\"input_ids\"].to(self.device), **kwargs)\n",
    "            decoded_emojis = self.tokenizer.decode(generated_emoji_tokens[0].cpu(), skip_special_tokens=True).replace(\" \", \"\")\n",
    "            decoded_emojis_list.append(decoded_emojis)\n",
    "            \n",
    "        return sep.join(decoded_emojis_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ea7fc789",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4720c08e754f4d53ba3407b60afc48e1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/401k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\nazmievairat\\anaconda3\\envs\\python312\\Lib\\site-packages\\huggingface_hub\\file_download.py:139: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\nazmievairat\\.cache\\huggingface\\hub\\models--AiratNazmiev--text2emoji-tokenizer. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "38952e84eaa34b6e87f052bc8e533a1d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.json:   0%|          | 0.00/1.05M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "06810652b710462db0be99d0faaa5ed2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "merges.txt:   0%|          | 0.00/506k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0eef5051a326400fb579dac91d6eee32",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "added_tokens.json:   0%|          | 0.00/43.2k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "db7c50bceb134335b7ced0944c23b2e9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/1.01k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cccf82cae2d34ce3944d1029d1477ea0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/1.80k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\nazmievairat\\anaconda3\\envs\\python312\\Lib\\site-packages\\huggingface_hub\\file_download.py:139: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\nazmievairat\\.cache\\huggingface\\hub\\models--AiratNazmiev--text2emoji-bart-base. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6ca040af831c411c8ed5c6384d70a897",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/565M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a3ed65ec272249bc8d95611af65ea77c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "generation_config.json:   0%|          | 0.00/305 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "tokenizer = BartTokenizer.from_pretrained('AiratNazmiev/text2emoji-tokenizer')\n",
    "generator = BartForConditionalGeneration.from_pretrained('AiratNazmiev/text2emoji-bart-base')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "1030a845",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cuda:0\n",
      "Device set to use cuda:0\n"
     ]
    }
   ],
   "source": [
    "ru_en_translator = pipeline(\n",
    "    \"translation_ru_to_en\", \n",
    "    model=\"Helsinki-NLP/opus-mt-ru-en\"\n",
    ")\n",
    "\n",
    "zh_en_translator = pipeline(\n",
    "    \"translation_zh_to_en\", \n",
    "    model=\"Helsinki-NLP/opus-mt-zh-en\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "24e4cd51",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_data_vis = tokenizer(\"To be or not to be. That's the question\", return_tensors=\"pt\")['input_ids']\n",
    "\n",
    "generator_graph = torchview.draw_graph(\n",
    "    generator.cpu(), \n",
    "    input_data=input_data_vis, \n",
    "    depth=3,\n",
    "    expand_nested=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "a5d5017f",
   "metadata": {},
   "outputs": [],
   "source": [
    "msg2emoji_translator = Msg2EmojiTranslator(\n",
    "    tokenizer=tokenizer,\n",
    "    generator=generator,\n",
    "    device=torch.device('cuda')\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "59c5931c",
   "metadata": {},
   "outputs": [],
   "source": [
    "twitter_magic_number = 280\n",
    "\n",
    "def text_preprocessing(text: str, language: str = 'en') -> str:\n",
    "    if language == 'ru':\n",
    "        text = ru_en_translator(text)[0]['translation_text']\n",
    "    elif language == 'zh':\n",
    "        text = zh_en_translator(text)[0]['translation_text']\n",
    "    \n",
    "    if len(text) > twitter_magic_number:\n",
    "        print(f\"It's twit translator. The max length of the input is {twitter_magic_number} characters\")\n",
    "        \n",
    "    text_re = re.split(r\"(?<=[.|!|?|\\.\\.\\.])\\s+\", text.strip())\n",
    "    \n",
    "    return text_re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "77235614",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[\"As I walk through the valley of the shadow of death\\nI take a look at my life and realize there's nothing left.\",\n",
       " \"Cause I've been blasting and laughing so long that\\nEven my momma thinks that my mind is gone\"]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentence = \"\"\"As I walk through the valley of the shadow of death\n",
    "I take a look at my life and realize there's nothing left.\n",
    "Cause I've been blasting and laughing so long that\n",
    "Even my momma thinks that my mind is gone\"\"\"\n",
    "\n",
    "sentence_re = text_preprocessing(sentence, language='en')\n",
    "sentence_re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "1a08fe2d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🚶‍♀️🏞️😔💔.😂👩‍👧‍👦💭\n"
     ]
    }
   ],
   "source": [
    "decoded = msg2emoji_translator.translate(\n",
    "    text_preprocessing(sentence),\n",
    "    sep='.',\n",
    "    num_beams=5, \n",
    "    do_sample=True, \n",
    "    max_length=20\n",
    ")\n",
    "print(fr'{decoded}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python312",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
